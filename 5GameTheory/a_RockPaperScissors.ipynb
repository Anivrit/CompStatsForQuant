{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fb92b6-235b-4e24-8839-fc2ce880ebc0",
   "metadata": {},
   "source": [
    "# Introduction to Game Theory\n",
    "\n",
    "[Game Theory](https://en.wikipedia.org/wiki/Game_theory) is the study of mathematical strategies among rational agents in a game. It has very important applications in financial markets, fundamentally due to the fact that every trade occurs between two parties who both believe their side of the trade to be the correct one. In this way, a market forms an effective 'game' with defined rules and millions of (approximately) rational agents participating. \n",
    "\n",
    "Understanding game theory principles such as [adverse selection](https://en.wikipedia.org/wiki/Adverse_selection) and [nash equilibria](https://en.wikipedia.org/wiki/Nash_equilibrium) are very useful concepts when designing strategies in quant trading. Studying games such as chess and poker are also good resources for learning ideas from game theory and supplement this course well. For a good introduction to poker theory see [The Course](https://books.google.co.uk/books/about/The_Course.html?id=GJSOrgEACAAJ&redir_esc=y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f12b7-0a0d-47da-9510-9c05a839e7dc",
   "metadata": {},
   "source": [
    "## Nash Equilibrium\n",
    "In this notebook we will look at one algorithm for solving the Nash equilibrium in the trivial example of rock paper scissors. In the next notebook we will look at a more interesting dice game which is inspired by an actual quant trading interview question.\n",
    "\n",
    "The concept of nash equilibrium essentially defines the solution to a non-cooperative game. It is a fairly nuanced idea and we recommend going through the wiki page or some textbooks before continuing. In summary, \"In a Nash equilibrium, each player is assumed to know the equilibrium strategies of the other players, and no one has anything to gain by changing only one's own strategy\".\n",
    "\n",
    "The Nash equilibrium for Rock Paper Scissors is known as a [mixed strategy](https://en.wikipedia.org/wiki/Strategy_(game_theory)#Mixed_strategy), and is simply to randomly pick each option randomly with probability $p=1/3$. Our aim is to reproduce this strategy computationally using [counterfactual regret minimization](https://poker.cs.ualberta.ca/publications/NIPS07-cfr.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e51bf-b105-4a58-be66-08eb451e920b",
   "metadata": {},
   "source": [
    "## Counterfactual Regret Minimization\n",
    "[Counterfactual regret minimization](https://poker.cs.ualberta.ca/publications/NIPS07-cfr.pdf) is an algorithm for solving nash equilibriums and is currently the state of the art for Texas No Limit Hold'em Poker solvers.\n",
    "\n",
    "Let $a$ be the actions available to player $i$. We define the *strategy*, $\\sigma_i(a)$, to be a probability distribution over the actions, such that $\\sum_{\\text{actions}} \\sigma_i(a) = 1$. If there are multiple turns in the game, then each turn will have a different strategy depending on the previous actions taken by other players. This defines a *strategy profile* for the player $\\sigma_i$.\n",
    "\n",
    "In the example of rock-paper-scissors both players have only one turn, and the actions are:\n",
    "* Choose Rock\n",
    "* Choose Paper\n",
    "* Choose Scissors\n",
    "\n",
    "Next we define the *utility*, $u_i(a)$, as the expected payoff for player $i$ for picking action $a$. Imagine that player A has a strategy where they pick rock 100% of the time. The utility for player B would look like $u_B(a) = \\{ R=0, P=1, S=-1 \\}$ for a $1 bet on each game.\n",
    "\n",
    "The algorithm works iteratively, starting with a random strategy for both players and slightly adjusting the strategies each iteration to eventually converge onto the nash equilibrium strategy.\n",
    "\n",
    "Define the immediate counterfactual *regret* on iteration $t$ of an action $a$ as $R_i^t(a)$. It is the difference between the utility of our strategy and the utility of picking action $a$ 100% of the time. \n",
    "$$\n",
    "r_i^t(a) = u_i(a) - \\sum_{a'} \\sigma_i(a') u_i(a')\n",
    "$$\n",
    "This quantifies how much regret we have for not choosing action $a$ as part of our strategy. If our regret is very positive, then we would want to add more of action $a$ to our strategy; and if the regret is very negative, then we would want to stop doing action $a$ as much.\n",
    "\n",
    "The accumulated regret is simply the sum of immediate regret over all iterations\n",
    "$$\n",
    "R^T_i(a) = \\sum_t^T r_i^t(a)\n",
    "$$\n",
    "To choose our strategy on iteration $t$ we use the accumulated regret. Let $R^{T,+}_i(a) = \\max(R^T_i(a),0)$ be the positive portion of accumulated regret.\n",
    "\n",
    "We choose the strategy for each player at iteration $T$ according to\n",
    "$$\n",
    "\\sigma^T_i(a) = \n",
    "\\frac{R^{T,+}_i(a)}{\\sum_{a} R^{T,+}_i(a)}.\n",
    "$$\n",
    "This has the nice property that the probabilities are always normalised over the actions, such that $\\sum_{\\text{actions}} \\sigma_i(a) = 1$. We also will choose $\\sigma^T_i(a) = 0$ for any action that has negative accumulated regret, as it is a very bad move.\n",
    "\n",
    "We must also consider the edge case in which $\\sum_a R^{T,+}_i(a)=0$ in which we will simply choose all strategies equally\n",
    "$$\n",
    "\\sigma^T_i(a) = \\frac{1}{A}\n",
    "$$\n",
    "where $A$ is the total number of actions.\n",
    "\n",
    "This now completes the counterfactual regret minimization algorithm. There is a proof that choosing strategies according to this scheme will converge to nash equilibrium as $T \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83c045-c621-4b40-99f8-cdbe362806ed",
   "metadata": {},
   "source": [
    "## Rock Paper Scissors\n",
    "Let us now implement this algorithm for rock paper scissors. We perform the CFR steps iteratively for each player. The hero is the current player who is updating their regrets taking the villains (other player's) strategy as an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3e889e-f532-4a93-8b9a-c46ea4fc7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445efaa6-d730-4118-9b83-54970cda7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represent actions by their index in the strategy array. Here we will use the convention \n",
    "rock     = 0\n",
    "paper    = 1\n",
    "scissors = 2\n",
    "'''\n",
    "\n",
    "def payoff(action: int, villain_strategy: np.array(float)) -> float:\n",
    "    '''\n",
    "    PAYOFF FUNCTION HERE\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db043961-83d5-47e4-ad3b-6f2329fb207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate immediate regret for every action\n",
    "'''\n",
    "\n",
    "def calculate_immediate_regret(hero_strategy: np.array(float), villain_strategy: np.array(float)) -> float:\n",
    "    '''\n",
    "    IMMEDIATE REGRET \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a8fadf3-756c-4fa3-9cce-6be98c6b7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate new strategy based on accumulated regret for the hero\n",
    "'''\n",
    "\n",
    "def calculate_strategy(acc_regrets: np.array(float)) -> np.array(float):\n",
    "    '''\n",
    "    CALCULATE NEW STRATEGY\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b614706e-47c4-4fd5-b885-7334c5229414",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform one CFR step. We return the new accumulated regrets.\n",
    "'''\n",
    "\n",
    "def cfr_step(hero_acc_regrets: np.array(float), villain_strategy: np.array(float)) -> np.array(float):\n",
    "    '''\n",
    "    RETURN NEW ACCUMULATED REGRETS FOR HERO\n",
    "    '''\n",
    "    return np.array([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c3b22b-5b82-4d01-8935-223b7a4187ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run CFR algorithm.\n",
    "We set initial strategy for player A and B to both pick rock 100% of the time. \n",
    "'''\n",
    "\n",
    "strategyA = np.array([1.0, 0.0, 0.0])\n",
    "strategyB = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "acc_regretsA = np.array([0.0, 0.0, 0.0])\n",
    "acc_regretsB = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "steps = 10\n",
    "\n",
    "for t in range(steps):\n",
    "    acc_regretsA = cfr_step(acc_regretsA, strategyB)\n",
    "    strategyA = calculate_strategy(acc_regretsA)\n",
    "    \n",
    "    acc_regretsB = cfr_step(acc_regretsB, strategyA)\n",
    "    strategyB = calculate_strategy(acc_regretsB)\n",
    "\n",
    "print(strategyA)\n",
    "print(strategyB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9665bc7-a115-48d1-af16-938e5af44f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
